{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1398369d",
   "metadata": {},
   "source": [
    "<h1> Natural Language Processing </h1>\n",
    "<p> Natural Language Processing (NLP) is the field of computing that tries to build programs that can understand, process and operate on human languages. The field is called \"natural\" languages is because these are <em> not </em> programming languages, and computers are quite bad at understanding them. </p>\n",
    "<p> To do this, we use something called a Recurrent Neural Network (RNN) which is much better at processing sequential data like text than the other models we have been working with. Some applications include machine translation or chatbots, although you can apply RNNs to anything that deals with textual data.</p>\n",
    "<p> We will apply RNNs to two applications: </p>\n",
    "<ul>\n",
    "    <li> Sentiment Analysis </li>\n",
    "    <li> Text generation </li>\n",
    "</ul>\n",
    "<h2> Encoding textual data to numerical data: Bag of Words </h2>\n",
    "<ul>\n",
    "    <li> Define the \"vocabulary\" of the neural network to be the set of unique words in its training data</li>\n",
    "    <li> Set these in a dictionary, where a number is set to each of these words.</li>\n",
    "    <li> Keep track of words present and frequency of words. I.e create a bag and add number encodings of word into the \"bag\" (an array). </li>\n",
    "    <li> I.e you lose the ordering of the words, but keep track of identiy and frequency. </li>\n",
    "    <li> Doesn't work so well for more complicated input. </li>\n",
    "</ul>\n",
    "<h2> Encoding textual data to numerical data: Word Embeddings </h2>\n",
    "<ul>\n",
    "    <li>Word embeddings encode words that have a similar meaning with similar numbers</li>\n",
    "    <li>It does this by classifying every single word into a vector in a large high dimensional space. (typically like ~64, or 128.</li>\n",
    "    <li>Words with similar meanings are close to eachother in the word-embedding space.</li>\n",
    "</ul>\n",
    "<h2> Recurrent Neural Networks </h2>\n",
    "<ul>\n",
    "    <li>Fundamental difference between a RNN and a vanilla dense neural network is the inclusion of an \"internal loop\". </li>\n",
    "    <li> The recurrant neural network doesn't process the entire input at once.</li>\n",
    "    <li> It processes it at different timesteps, maintaining an \"internal memory\" so that when it looks at a new piece of the input, it will remember what it has seen previously and will treat the input based on the understanding it has already developed of the data. This is analogous to how humans read a paragraph. </li>\n",
    "    <li>This is in contrast to standard \"feed-forward\" neural nets, in which data is passed from left to right through the neural network, in one direction: forward.</li>\n",
    "    <li> Take a look at the following diagram. At step n, the reccurent layer (A) takes an input $x_{n}$ and produces an output $h_{n}$. The output is then integrated into the layer (A's) analysis of the next input $x_{n+1}$. </li>\n",
    "<img src=\"./imgs/rnn_diagram.png\">\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea5f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
